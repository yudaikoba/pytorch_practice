{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テンソル：Tensors \n",
    "==========================\n",
    "\n",
    "テンソルは特殊なデータ構造で、配列や行列によく似ています。\n",
    "\n",
    "PyTorchではテンソル型の変数を使用して、モデルの入力と出力、そしてモデルのパラメータを表現します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テンソルは[NumPy](https://numpy.org/)のndarraysに似ていますが、違いとしてGPUや他のハードウェアアクセラレータ上で動作させることができます。\n",
    "\n",
    "テンソルとNumPyの配列は基本的には同じメモリを共有することができるため、2つの型間での変換時にはデータをコピーする必要がありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "テンソルはその他に、自動微分に最適化されています（この点については、後ほど5. 自動微分にて、詳しく説明します）。\n",
    "\n",
    "NumPyのndarraysに慣れている人は、Tensor APIをすぐに使いこなせると思います。\n",
    "\n",
    "そうでない場合には、本チュートリアルを通してぜひ習得してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テンソルの初期化\n",
    "=======================\n",
    "\n",
    "テンソルは様々な手法で初期化できます。\n",
    "\n",
    "以下に例を示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**データから直接テンソルに変換**\n",
    "\n",
    "データから直接テンソルを作ることができます。\n",
    "\n",
    "その際、データ型は自動的に推測されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NumPy arrayからテンソルに変換**\n",
    "\n",
    "テンソルとNumpy arraysは相互に変換可能です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**他のテンソルから作成**\n",
    "\n",
    "他のテンソルから新しいテンソルを作成する場合、明示的に上書きされない限り、引数のテンソルのプロパティ（形状、データ型）を保持します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.6551, 0.2417],\n",
      "        [0.5058, 0.5717]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # x_dataの特性を維持\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # x_dataのdatatypeを上書き\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ランダム値や定数のテンソルの作成**\n",
    "\n",
    "\n",
    "``shape``は、テンソルの次元を示すタプルです。\n",
    "\n",
    "以下の例では、shapeからテンソルのサイズを決めています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.6993, 0.9583, 0.7754],\n",
      "        [0.6162, 0.9553, 0.1885]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor  = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テンソルの操作\n",
    "==================\n",
    "\n",
    "PyTorchでは、算術、線形代数、行列操作（転置、インデックス、スライス）など、100種類以上のテンソル演算が可能です。\n",
    "\n",
    "種々操作の詳細は[こちら]((https://pytorch.org/docs/stable/torch.html)\n",
    ")をご覧ください。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各操作はGPU上で実行可能です（一般的にCPUの場合より高速です）。\n",
    "\n",
    "\n",
    "Google Colaboratoryを使用している場合は、GPUを使用できるように設定してください。\n",
    "\n",
    "（Runtime > Change runtime type > GPU）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デフォルトではテンソルはCPU上で作られます。\n",
    "\n",
    "そのため、明示的に、``.to``メソッドを使用して、テンソルをGPU上へと移動させます。\n",
    "\n",
    "大きなテンソルをデバイス間でコピーすると、時間とメモリの面でコストがかかる点にご注意ください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPUが使用可能であれば、GPU上にテンソルを移動させる\n",
    "tensor = torch.ones(4, 4)\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テンソル操作の中からいくつかを試してみましょう。\n",
    "\n",
    "NumPy APIに慣れていれば、Tensor APIも簡単に使えるようになると思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**numpy-likeなindexingとslicing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1., 1., 1., 1.])\n",
      "First column:  tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print('First row: ',tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `:` は特定の次元をスライスするために使う。\n",
    "- `...` は任意の数の次元をスキップするために使い、特に高次元テンソルで便利。\n",
    "\n",
    "例えば、3次元テンソルやそれ以上の次元のテンソルを扱うとき、`...`を使うことで簡潔に指定できるんや："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**テンソルの結合** \n",
    "\n",
    "\n",
    "``torch.cat``を使用することで、テンソルを特定の次元に沿って結合させることができます（詳細は[こちら](https://pytorch.org/docs/stable/generated/torch.stack.html)をご覧ください）。\n",
    "\n",
    "``torch.cat``とは微妙に異なるテンソル結合演算である[``torch.stack``](https://pytorch.org/docs/stable/generated/torch.stack.html)も確認しておいてください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cat result:\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "torch.stack result:\n",
      " tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# torch.catの場合\n",
    "cat_result = torch.cat((tensor1, tensor2), dim=0)\n",
    "print('torch.cat result:\\n', cat_result)\n",
    "\n",
    "# torch.stackの場合\n",
    "stack_result = torch.stack((tensor1, tensor2), dim=0)\n",
    "print('torch.stack result:\\n', stack_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**算術演算**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2つのテンソル行列のかけ算です。 y1, y2, y3 は同じ結果になります。\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# こちらは、要素ごとの積を求めます。 z1, z2, z3 は同じ値になります。\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1要素のテンソル** \n",
    "\n",
    "1要素のテンソル（テンソルの全要素を足し算する等をした結果生まれます）を扱う場合には、``.item()``を使用することでPythonの数値型変数に変換できます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()  \n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**インプレース操作**\n",
    "\n",
    "\n",
    "演算結果をオペランドに格納する演算をインプレースと呼びます。\n",
    "\n",
    "メソッドの最後、接尾辞として操作名に、 ``_`` が付きます。\n",
    "\n",
    "例えば、``x.copy_(y)``, ``x.t_()``であり、``x``の内容そのものを更新します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【注意】\n",
    "\n",
    "\n",
    "インプレース操作はメモリを節約できますが、演算履歴が失われてしまうため、微分を計算する際には問題となります。\n",
    "\n",
    "そのため、そのような微分を求める場面ではインプレース操作の使用は推奨されていません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPyとの変換\n",
    "=========================\n",
    "\n",
    "CPU上のテンソルとNumpy arraysは同じメモリを共有することができ、相互変換が容易です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor to NumPy array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この際、テンソルが変化すると、Numpy側も変化します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NumPy array to Tensor**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy arrayの変化はテンソル側にも反映されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
